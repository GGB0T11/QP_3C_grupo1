# ğŸ“Œ Conceitos Fundamentais
O **clustering** Ã© uma tÃ©cnica de aprendizado de mÃ¡quina **nÃ£o supervisionado** usada para identificar padrÃµes ocultos em dados nÃ£o rotulados. Ele agrupa elementos similares e separa os que possuem caracterÃ­sticas distintas.

A base matemÃ¡tica do clustering envolve **mÃ©tricas de distÃ¢ncia**, como a **Euclidiana** e a de **Manhattan**, para medir a similaridade entre pontos. O objetivo central do clustering Ã©:
- **Minimizar** a variabilidade dentro dos clusters.
- **Maximizar** a diferenÃ§a entre eles.

---

# ğŸš€ Algoritmos de Clustering

## ğŸ”¹ K-Means (Baseado em PartiÃ§Ã£o)
O **K-Means** Ã© um dos algoritmos de clustering mais usados, baseado na divisÃ£o dos dados em **k grupos (partiÃ§Ãµes)**. Cada grupo Ã© representado pelo seu **centrÃ³ide**, que Ã© o ponto mÃ©dio de todos os elementos do cluster.

### ğŸ›  Como Funciona?
1. Definir o **nÃºmero de clusters k**.
2. Selecionar **k pontos aleatÃ³rios** como centroides iniciais.
3. Atribuir cada ponto ao centrÃ³ide mais prÃ³ximo (usando a **distÃ¢ncia Euclidiana**).
4. Atualizar os centroides calculando a **mÃ©dia dos pontos** em cada cluster.
5. Repetir os passos **3 e 4** atÃ© que os centroides parem de mudar (**convergÃªncia**).

### âœ… Vantagens
âœ” Simples e eficiente para grandes conjuntos de dados.  
âœ” Funciona bem em dados esfÃ©ricos e bem separados.

### âŒ Desvantagens
âœ– Precisa definir **k previamente**.  
âœ– SensÃ­vel a **valores atÃ­picos** e clusters de formas irregulares.

---

## ğŸ”¹ DBSCAN (Baseado em Densidade)
O **DBSCAN** forma clusters com base na **densidade dos pontos** e pode **detectar outliers automaticamente**.

### ğŸ›  Como Funciona?
1. Define um **raio de vizinhanÃ§a (eps)** e um **nÃºmero mÃ­nimo de pontos (min_samples)**.
2. Identifica **pontos centrais** (com pelo menos **min_samples vizinhos** dentro de **eps**).
3. Expande clusters a partir desses **pontos centrais**, incluindo vizinhos prÃ³ximos.
4. Pontos que nÃ£o pertencem a nenhum cluster sÃ£o marcados como **outliers (-1)**.

### âœ… Vantagens
âœ” Detecta clusters de **formas irregulares**.  
âœ” Identifica **outliers automaticamente**.

### âŒ Desvantagens
âœ– SensÃ­vel Ã  escolha de **eps e min_samples**.  
âœ– Pode ter dificuldades em **dados de alta dimensÃ£o**.

---

## ğŸ”¹ Clustering HierÃ¡rquico
O **clustering hierÃ¡rquico** cria uma estrutura de Ã¡rvore (**dendrograma**) para organizar os dados. Ele pode ser de dois tipos:
- **Aglomerativo (Bottom-Up):** ComeÃ§a com cada ponto separado e vai unindo os mais prÃ³ximos atÃ© formar um Ãºnico cluster.
- **Divisivo (Top-Down):** ComeÃ§a com um Ãºnico cluster e vai dividindo em subgrupos.

### ğŸ›  Como Funciona? (MÃ©todo Aglomerativo)
1. Cada ponto comeÃ§a como um **cluster individual**.
2. Os **clusters mais prÃ³ximos** sÃ£o mesclados com base na **distÃ¢ncia**.
3. O processo continua atÃ© que reste **apenas um cluster** contendo todos os pontos.
4. Podemos **cortar o dendrograma** para obter o nÃºmero desejado de clusters.

### âœ… Vantagens
âœ” NÃ£o exige definir o **nÃºmero de clusters previamente**.  
âœ” Gera um **dendrograma Ãºtil** para anÃ¡lise exploratÃ³ria.

### âŒ Desvantagens
âœ– Computacionalmente **mais pesado** que K-Means para grandes conjuntos de dados.  
âœ– NÃ£o lida bem com **outliers**.
